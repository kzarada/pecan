---
title: "SDA Walkthrough"
author: "K. Zarada"
date: "5/21/2020"
output: html_document
---

This is a basic walk through the Willow Creek SDA forecast. We currently have two workflows that I am working with but I am more concerned with you being familiar with the SDA_testing_workflow.R (which is in the modules/assim.sequential/inst/WillowCreek/ folder in PEcAn). 

The SDA is an extension of our exisiting Willow Creek SIPNET forecast which you can see on the [flux dashboard](http://test-pecan.bu.edu/shiny/Flux_Dashboard/). This SIPNET forecast uses NOAA GEFS meterology data. [NOAA GEFS](https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-ensemble-forecast-system-gefs) is a 16 day, 21 ensemble member forecast. You can also look at the NOAA GEFS ensembles on the NOAA GEFS tab of the flux dashboard. 

With the SDA, we are trying to update the previous forecast (instead of starting over each day like we do with the exisiting SIPTNET forecast) and incorporate other data streams into the forecast to make better predictions. So far, we have incorporated LAI. The next step is to bring in soil moisture, which is what I want you to work on this summer. 

We'll start this process by downloading the soil moisture data from the Willow Creek tower and validating the forecasted soil moisture. I already have a script for the soil moisture download, but it would be good for you to look through it and possibilty update it (I wrote it a while ago). 

Before we get to that though, let's go through the SDA and how to manually run it in PEcAn. This RMD doesn't have live code, so you should read it as you go through the SDA_test_workflow on your Rstudio. This workflow is where I test and debug the forecast. It only runs the forecast for a couple of days instead of the full 16. **NOTE** The SDA takes a really long time to run. It depends on how many time steps we are doing it for, but a three day run done hourly can take > 7 hours. 

```
outputPath <- "/fs/data3/kzarada/ouput/LAI_Reanalysis/"
nodata <- FALSE #use this to run SDA with no data
restart <-TRUE #flag to start from previous run or not
days.obs <- 1  #how many of observed data *BY HOURS* to include -- not including today
setwd(outputPath)
options(warn=-1)

``` 

This first chunk of code names our *output path* (you can change this to wherever you are going to do runs in your working directory). We have a flag for *nodata* which would mean that we aren't inputing anything (this is not needed for this workflow). The flag for *restart* tells the SDA if we want it to start from the previous run or if we want it to start fresh. Our goal is to have it always running a restart, but sometimes it is easier to get something running without the restart and then debug the restart. *days.obs* tells how many **hours** we want to be observed before the forecast start. It just recently changed from days to hours which is why it's still *days.obs* If we are adding in more data, it might be useful to have more days observed but adding observed days will take away from the number of days we can forecast because the total number of days from the start of the obs.data to the sda.end have to be within 16 days to line up with the met data (remember that NOAA GEFS data is only 16 days long). 


```
c(
  'Utils.R',
  'download_WCr.R',
  "gapfill_WCr.R",
  'prep.data.assim.R'
) %>% walk( ~ source(
  system.file("WillowCreek",
              .x,
              package = "PEcAn.assim.sequential")
))


#------------------------------------------------------------------------------------------------
#------------------------------------------ Preparing the pecan xml -----------------------------
#------------------------------------------------------------------------------------------------

  #reading xml
  settings <- read.settings("/fs/data3/kzarada/pecan/modules/assim.sequential/inst/WillowCreek/testing.xml")

```

This next part downloads some necessary functions to get the Willow Creek flux data from the tower. Then we read the xml. The xml is how we tell PEcAN what we want. The xml is very important and can impact a lot.

For the testing workflow, we are using `testing.xml`. Most of the xml is actually updated in the SDA workflow - like the start dates and such- but if we want to change things like the ensemble number (10 in the testing xml, usually > 100) or add or remove variables, we have to do it in the xml. You can read more about the xml in PEcAn [here.](https://pecanproject.github.io/pecan-documentation/develop/pecanXML.html)


The next section finds that last SDA run in the output folder (that completed) and gets the date and file path from it and then makes the sda.start the day following. It looks into our output folder and gets every directory within the folder. It then removes any directory that doesn't have an sda.output.Rdata because these runs did not finish. It finds the start date from the last simulation and adds 3 days to make a new start date. We're adding three days here because the met.start day is saved in BETY (our SQL database) and our met data is 2 days behind the sda.start date AND we want the sda to run for the next day- making 3. We then get a new `sda.end` and `met.start` and `met.end`. The days added to the sda.end will impact how long our forecast is, so as we get better forecasts, we should increase that to get more. BUT we have to be careful not to exceed the length of the met data. 

```
  #connecting to DB
  con <-try(PEcAn.DB::db.open(settings$database$bety), silent = TRUE)

  #Find last SDA Run to get new start date 
  all.previous.sims <- list.dirs(outputPath, recursive = F)
  if (length(all.previous.sims) > 0 & !inherits(con, "try-error")) {
    
    tryCatch({
      # Looking through all the old simulations and find the most recent
      all.previous.sims <- all.previous.sims %>%
        map(~ list.files(path = file.path(.x, "SDA"))) %>%
        setNames(all.previous.sims) %>%
        discard( ~ !"sda.output.Rdata" %in% .x) # I'm throwing out the ones that they did not have a SDA output
      
      last.sim <-
        names(all.previous.sims) %>%
        map_chr( ~ strsplit(.x, "_")[[1]][3]) %>%
        map_dfr(~ db.query(
          query = paste("SELECT * FROM workflows WHERE id =", .x),
          con = con
        ) %>%
          mutate(ID=.x)) %>%
        mutate(start_date = as.Date(start_date)) %>%
        arrange(desc(start_date), desc(ID)) %>%
        head(1)
      # pulling the date and the path to the last SDA
      restart.path <-grep(last.sim$ID, names(all.previous.sims), value = T)
      sda.start <- last.sim$start_date+ lubridate::days(3)
    },
    error = function(e) {
      restart.path <- NULL
      sda.start <- Sys.Date() - 9
      PEcAn.logger::logger.warn(paste0("There was a problem with finding the last successfull SDA.",conditionMessage(e)))
    })
    
    # if there was no older sims
    if (is.na(sda.start))
      sda.start <- Sys.Date() - 9
  }
  #to manually change start date 
  #sda.start <-  as.POSIXct("2019-06-20", tz = "UTC")
  sda.end <- sda.start + lubridate::days(2)
  
  # Finding the right end and start date
  met.start <- sda.start - lubridate::days(2)
  met.end <- met.start + lubridate::days(16)
```


Next we download the existing flux data. This uses the prep.data.assim, gapfill_WCr, and download_WCr functions that we sourced in earlier. The download_WCr gets the Flux data from where it is saved online. The gapfill uses a function to fill any missing data in the Fluxes because that can cause issues, and the prep.data preps is where we change the units if we want (I currently am not changing the units because I want them to be larger numbers for the matrix algebra that occurs in the analysis step). Prep.data also calculates the covariance and the means. We then remove any NA's because that messes things up and we change 'LE' to 'Qle' because SIPNET looks for 'Qle'. 

```
#-----------------------------------------------------------------------------------------------
  #------------------------------------------ Download met and flux ------------------------------
  #-----------------------------------------------------------------------------------------------
  #Fluxes
  prep.data <- prep.data.assim(
    sda.start - lubridate::days(90),# it needs at least 90 days for gap filling 
    sda.end,
    numvals = 100,
    vars = c("NEE", "LE"),
    data.len = days.obs, 
    sda.start)
  
  obs.raw <-prep.data$rawobs
  prep.data<-prep.data$obs
  

  # if there is infinte value then take it out - here we want to remove any that just have one NA in the observed data 
  prep.data <- prep.data %>% 
    map(function(day.data){
      #cheking the mean
      nan.mean <- which(is.infinite(day.data$means) | is.nan(day.data$means) | is.na(day.data$means))
      if ( length(nan.mean)>0 ) {
        
        day.data$means <- day.data$means[-nan.mean]
        day.data$covs <- day.data$covs[-nan.mean, -nan.mean] %>%
          as.matrix() %>%
          `colnames <-`(c(colnames(day.data$covs)[-nan.mean]))
      }
      day.data
    })
  
  
  # Changing LE to Qle which is what SIPNET expects
  prep.data <- prep.data %>%
    map(function(day.data) {
      names(day.data$means)[names(day.data$means) == "LE"] <- "Qle"
      dimnames(day.data$covs) <- dimnames(day.data$covs) %>%
        map(function(name) {
          name[name == "LE"] <- "Qle"
          name
        })
      
      day.data
    })
  
```


Next, we download LAI data from MODIS. This isn't all that important for you to know. I want you to instead look at how we incoproate LAI data with the flux data to get it to the SDA. When you are prepping data to be added to the SDA, you need to make sure that we are comparing apples to apples. So the units need to be what SIPNET expects and it needs to match up with the output of SIPNET for the validation step. 

```
# --------------------------------------------------------------------------------------------------
  #---------------------------------------------- LAI DATA -------------------------------------
  # --------------------------------------------------------------------------------------------------
  
  site_info <- list(
    site_id = 676,
    site_name = "Willow Creek",
    lat = 45.805925,
    lon = -90.07961,
    time_zone = "UTC")
  
  tryCatch({
    lai <- call_MODIS(outdir = NULL,
                      var = 'lai', 
                      site_info = site_info, 
                      product_dates = c(paste0(lubridate::year(met.start), strftime(met.start, format = "%j")),paste0(lubridate::year(met.end), strftime(met.end, format = "%j"))),
                      run_parallel = TRUE, 
                      ncores = NULL, 
                      product = "MOD15A2H", 
                      band = "Lai_500m",
                      package_method = "MODISTools", 
                      QC_filter = TRUE,
                      progress = TRUE)
    lai <- lai %>% filter(qc == "000")}, 
    error = function(e) {
      lai <- NULL
      PEcAn.logger::logger.warn(paste0("MODIS Data not available for these dates",conditionMessage(e)))
    }
  )
  if(!exists('lai')){lai = NULL}
  
  
  tryCatch({
    lai_sd <- call_MODIS(outdir = NULL,
                         var = 'lai', 
                         site_info = site_info, 
                         product_dates = c(paste0(lubridate::year(met.start), strftime(met.start, format = "%j")),paste0(lubridate::year(met.end), strftime(met.end, format = "%j"))),
                         run_parallel = TRUE, 
                         ncores = NULL, 
                         product = "MOD15A2H", 
                         band = "LaiStdDev_500m",
                         package_method = "MODISTools", 
                         QC_filter = TRUE,
                         progress = TRUE)
    lai_sd <- lai_sd %>% filter(qc == "000")}, 
    error = function(e) {
      lai_sd <- NULL
      PEcAn.logger::logger.warn(paste0("MODIS Data not available for these dates",conditionMessage(e)))
    }
  )
  if(!exists('lai_sd')){lai_sd = NULL}
  
```

Here, we pad the observed data so we can create a forecast. We put this into the SDA with NA's so it will forecast. We can change the date sequence to days or different hours to change our time step for the forecast.



```

###### Pad Observed Data to forecast ############# 
  
  date <-
    seq(
      from = lubridate::force_tz(as.POSIXct(last(names(prep.data)), format = "%Y-%m-%d %H:%M:%S"), tz = "UTC") + lubridate::hours(1),
      to = lubridate::with_tz(as.POSIXct(first(sda.end) + lubridate::days(1), format = "%Y-%m-%d %H:%M:%S"), tz = "UTC"),
      by = "1 hour"
    )
  
  pad.prep <- obs.raw %>%
    tidyr::complete(Date = date) %>%
    mutate(means = NA, covs = NA) %>%
    dplyr::select(Date, means, covs) %>%
    dynutils::tibble_as_list()
  
  names(pad.prep) <-date
  
```

This is a section I want you to pay particular attention to. This is where I add in the LAI data to the observed data. We need to add it to the means and also add in a covariance matrix. For now, we are having LAI have 0 covariance with NEE and Qle. We'll have to discuss what to put for any other data that we put in. 

```
 #Add in LAI info 
  
  if(is.null(lai)){index <- rep(FALSE, length(names(prep.data)))}else{
    index <- as.Date(names(prep.data)) %in% as.Date(lai$calendar_date)
  }
  
  
  for(i in 1:length(index)){
    
    if(index[i]){
      lai.date <- which(as.Date(lai$calendar_date) %in% as.Date(names(prep.data)))
      LAI <- c(0,0)
      prep.data[[i]]$means <- c(prep.data[[i]]$means, lai$data[lai.date])
      prep.data[[i]]$covs <- rbind(cbind(prep.data[[i]]$covs, c(0, 0)), c(0,0, lai_sd$data))
      
      names(prep.data[[i]]$means) <- c("NEE", "Qle", "LAI")
      rownames(prep.data[[i]]$covs) <- c("NEE", "Qle", "LAI")
      colnames(prep.data[[i]]$covs) <- c("NEE", "Qle", "LAI")
      
    }
  }
```

Here we combine the observed data with the padded forecasted dates and then split the data into obs.mean and obs.cov

```
  
#add forecast pad to the obs data  
    prep.data = c(prep.data, pad.prep)
  
#split into means and covs 
    
  obs.mean <- prep.data %>%
                map('means') %>% 
                setNames(names(prep.data))
  obs.cov <- prep.data %>% map('covs') %>% setNames(names(prep.data))
  
```

The next section fixes the settings in the xml and starts the runs to prep for the SDA. This is a normal part of the PEcAn workflow

```
  
  #-----------------------------------------------------------------------------------------------
  #------------------------------------------ Fixing the settings --------------------------------
  #-----------------------------------------------------------------------------------------------
  #unlink existing IC files
  sapply(paste0("/fs/data3/kzarada/pecan.data/dbfiles/IC_site_0-676_", 1:100, ".nc"), unlink)
  #Using the found dates to run - this will help to download mets
  settings$run$start.date <- as.character(met.start)
  settings$run$end.date <- as.character(met.end)
  settings$run$site$met.start <- as.character(met.start)
  settings$run$site$met.end <- as.character(met.end)
  #info
  settings$info$date <- paste0(format(Sys.time(), "%Y/%m/%d %H:%M:%S"), " +0000")
  
  # --------------------------------------------------------------------------------------------------
  #---------------------------------------------- PEcAn Workflow -------------------------------------
  # --------------------------------------------------------------------------------------------------
  #Update/fix/check settings. Will only run the first time it's called, unless force=TRUE
  settings <- PEcAn.settings::prepare.settings(settings, force=FALSE)
  setwd(settings$outdir)
  ggsave(
    file.path(settings$outdir, "Obs_plot.pdf"),
    ploting_fluxes(obs.raw) ,
    width = 16,
    height = 9
  )
  
  #Write pecan.CHECKED.xml
  PEcAn.settings::write.settings(settings, outputfile = "pecan.CHECKED.xml")
  # start from scratch if no continue is passed in
  statusFile <- file.path(settings$outdir, "STATUS")
  if (length(which(commandArgs() == "--continue")) == 0 && file.exists(statusFile)) {
    file.remove(statusFile)
  }
  # Do conversions
  settings <- PEcAn.workflow::do_conversions(settings, T, T, T)
  
  # Query the trait database for data and priors
  if (PEcAn.utils::status.check("TRAIT") == 0) {
    PEcAn.utils::status.start("TRAIT")
    settings <- PEcAn.workflow::runModule.get.trait.data(settings)
    PEcAn.settings::write.settings(settings, outputfile = 'pecan.TRAIT.xml')
    PEcAn.utils::status.end()
  } else if (file.exists(file.path(settings$outdir, 'pecan.TRAIT.xml'))) {
    settings <-
      PEcAn.settings::read.settings(file.path(settings$outdir, 'pecan.TRAIT.xml'))
  }
  # Run the PEcAn meta.analysis
  if (!is.null(settings$meta.analysis)) {
    if (PEcAn.utils::status.check("META") == 0) {
      PEcAn.utils::status.start("META")
      PEcAn.MA::runModule.run.meta.analysis(settings)
      PEcAn.utils::status.end()
    }
  }
  #sample from parameters used for both sensitivity analysis and Ens
  get.parameter.samples(settings, ens.sample.method = settings$ensemble$samplingspace$parameters$method)
  # Setting dates in assimilation tags - This will help with preprocess split in SDA code
  settings$state.data.assimilation$start.date <-as.character(first(names(obs.mean)))
  settings$state.data.assimilation$end.date <-as.character(last(names(obs.mean)))
  
  if (nodata) {
    obs.mean <- obs.mean %>% map(function(x)
      return(NA))
    obs.cov <- obs.cov %>% map(function(x)
      return(NA))
  }
```

The last part of this makes the obs.mean/cov NA if we want to do a nodata run. 

This next section preps for the restart. We want the ANALYSIS and FORECAST sections to line up with our obs.mean/cov and to be our first time step. This gets the forecast to start from our predictions yesterday instead of starting over. 

```
# --------------------------------------------------------------------------------------------------
  #--------------------------------- Restart -------------------------------------
  # --------------------------------------------------------------------------------------------------
  
  if(restart == TRUE){
    if(!dir.exists("SDA")) dir.create("SDA",showWarnings = F)
    
    #Update the SDA Output to just have last time step 
    temp<- new.env()
    load(file.path(restart.path, "SDA", "sda.output.Rdata"), envir = temp)
    temp <- as.list(temp)
    
    #we want ANALYSIS, FORECAST, and enkf.parms to match up with how many days obs data we have
    # +24 because it's hourly now and we want the next day as the start 
    if(length(temp$ANALYSIS) > 1){
      
      for(i in 1:days.obs + 1){ 
        temp$ANALYSIS[[i]] <- temp$ANALYSIS[[i + 24]]
          }
      for(i in rev((days.obs + 2):length(temp$ANALYSIS))){ 
        temp$ANALYSIS[[i]] <- NULL
      }
      
      
      for(i in 1:days.obs + 1){ 
        temp$FORECAST[[i]] <- temp$FORECAST[[i + 24]]
      }
      for(i in rev((days.obs + 2):length(temp$FORECAST))){ 
        temp$FORECAST[[i]] <- NULL
      }
    
      for(i in 1:days.obs + 1){ 
        temp$enkf.params[[i]] <- temp$enkf.params[[i + 24]]
      }
      for(i in rev((days.obs + 2):length(temp$enkf.params))){ 
        temp$enkf.params[[i]] <- NULL
      }    
      
    }
    temp$t = 1 
    
    #change inputs path to match sampling met paths 
    
    for(i in 1: length(temp$inputs$ids)){
      
      temp$inputs$samples[i] <- settings$run$inputs$met$path[temp$inputs$ids[i]]
      
    }
    
    temp1<- new.env()
    list2env(temp, envir = temp1)
    save(list = c("ANALYSIS", 'FORECAST', "enkf.params", "ensemble.id", "ensemble.samples", 'inputs', 'new.params', 'new.state', 'run.id', 'site.locs', 't', 'Viz.output', 'X'),
         envir = temp1, 
         file = file.path(settings$outdir, "SDA", "sda.output.Rdata"))  
    
    
    
    temp.out <- new.env()
    load(file.path(restart.path, "SDA", 'outconfig.Rdata'), envir = temp.out)
    temp.out <- as.list(temp.out)
    temp.out$outconfig$samples <- NULL
    
    temp.out1 <- new.env()
    list2env(temp.out, envir = temp.out1)
    save(list = c('outconfig'), 
         envir = temp.out1, 
         file = file.path(settings$outdir, "SDA", "outconfig.Rdata"))
    
    
    
    #copy over run and out folders 
    
    if(!dir.exists("run")) dir.create("run",showWarnings = F)
    
    files <- list.files(path = file.path(restart.path, "run/"), full.names = T, recursive = T, include.dirs = T, pattern = "sipnet.clim")
    readfiles <- list.files(path = file.path(restart.path, "run/"), full.names = T, recursive = T, include.dirs = T, pattern = "README.txt")
    
    newfiles <- gsub(pattern = restart.path, settings$outdir, files)
    readnewfiles <- gsub(pattern = restart.path, settings$outdir, readfiles)
    
    rundirs <- gsub(pattern = "/sipnet.clim", "", files)
    rundirs <- gsub(pattern = restart.path, settings$outdir, rundirs)
    for(i in 1 : length(rundirs)){
      dir.create(rundirs[i]) 
      file.copy(from = files[i], to = newfiles[i])
      file.copy(from = readfiles[i], to = readnewfiles[i])} 
    file.copy(from = paste0(restart.path, '/run/runs.txt'), to = paste0(settings$outdir,'/run/runs.txt' ))
    
    if(!dir.exists("out")) dir.create("out",showWarnings = F)
    
    files <- list.files(path = file.path(restart.path, "out/"), full.names = T, recursive = T, include.dirs = T, pattern = "sipnet.out")
    newfiles <- gsub(pattern = restart.path, settings$outdir, files)
    outdirs <- gsub(pattern = "/sipnet.out", "", files)
    outdirs <- gsub(pattern = restart.path, settings$outdir, outdirs)
    for(i in 1 : length(outdirs)){
      dir.create(outdirs[i]) 
      file.copy(from = files[i], to = newfiles[i])} 
    
  }
```


And, finally. We have the function to run the SDA. I would recommend changing debug to = TRUE so you can walk through the function manually to see what all is going on. You can also look through the function in 'pecan/modules/assim.sequential/R/sda.enfk_refactored.R' to see what is going on. 

```
#if(restart == FALSE) unlink(c('run','out','SDA'), recursive = T)
  
  if ('state.data.assimilation' %in% names(settings)) {
    if (PEcAn.utils::status.check("SDA") == 0) {
      PEcAn.utils::status.start("SDA")
      PEcAn.assim.sequential::sda.enkf(
        settings, 
        restart=restart,
        Q=0,
        obs.mean = obs.mean,
        obs.cov = obs.cov,
        control = list(
          trace = TRUE,
          interactivePlot =FALSE,
          TimeseriesPlot =TRUE,
          BiasPlot =FALSE,
          debug = FALSE,
          pause=FALSE
        )
      )
      
      PEcAn.utils::status.end()
    }
  }
```


Okay so this is just a basic run down of what is going on in the SDA Forecast workflow. There is a lot going on here and a lot to learn, so play with this and let me know what questions you have with it. Once you are more comfortable with this workflow, we'll dive into the soil moisture download. 